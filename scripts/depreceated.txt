#I am working
# def read_pickles(*argv): 
# 	#head,tail = os.path.split(raster_2)
# 	actual_file = argv[9]+f'{argv[5]}_{argv[10]}_zonal_stats_df'
# 	predicted_file = argv[9]+f'{argv[7]}_{argv[6]}_{argv[10]}_zonal_stats_df'
# 	if argv[8].lower()=='true': 
# 		print('pickling...')
# 		#generate zonal stats and pickle
		
# 		if not os.path.exists(predicted_file): 
# 			print(f'creating new files {actual_file} and {predicted_file}')
# 			actual = calc_zonal_stats(argv[0],argv[2],argv[3],argv[4],argv[5])
# 			predicted = calc_zonal_stats(argv[1],argv[2],argv[3],argv[4],argv[6])
# 			#df = pd.concat([stem_df,rgi_df],axis=1)
# 			actual_ds = pickle.dump(actual, open(actual_file, 'ab' ))
# 			predicted_ds = pickle.dump(predicted,open(predicted_file,'ab'))
# 			return actual, predicted 
# 		else: 
# 			print('both files already exist')
# 			pass
# 	else: #read in the pickled df if its the same data
# 		print('reading from pickle...')
# 		actual_df = pickle.load(open(actual_file,'rb'))
# 		predicted_df = pickle.load(open(predicted_file,'rb'))
# 		#print(actual_df.head())
# 		return actual_df,predicted_df
# #argv order: #0:nlcd_raster,1:stem_raster,2:random_pts,3:resolution,4:stat,5:actual_source,6:predicted_source,7:model_run,8:write_to_pickle,9:pickle_dir,10:modifier)

# def calc_confusion_matrix(*argv):#actual_source,predicted_source,stat,): 
# 	"""Calculate a confusion matrix to compare nlcd or rgi and classification."""
# 	data = read_pickles(*argv)
# 	actual = data[0].replace(np.nan,0)#read_pickles(*argv)[0].replace(np.nan,0)
# 	predicted = data[1].replace(np.nan,0)#read_pickles(*argv)[1].replace(np.nan,0)#calc_zonal_stats(predicted_raster,shp,resolution,stat,predicted_source)
# 	#print(actual.head())
# 	#print(predicted.head())
# 	actual_col = actual[str(argv[5]+'_'+argv[4])]
# 	predicted_col = predicted[str(argv[6]+'_'+argv[4])]
# 	print(actual_col.unique())
# 	print(predicted_col.unique())
# 	actual_ls = [float(i) for i in list(actual_col)]
# 	predicted_ls = [float(i) for i in list(predicted_col)]
# 	labels = sorted(list(set(list(actual_col)+list(predicted_col))))# sorted(actual[str(argv[5]+'_'+argv[4])].unique())
# 	print(labels)
# 	results = confusion_matrix(actual_ls, predicted_ls,labels) 
# 	#disp = plot_confusion_matrix(None,actual_ls,predicted_ls,display_labels=labels,cmap=plt.cm.Blues)
# 	#fig,(ax,ax1) = plt.subplots(nrows=1,ncols=2)
# 	ax=plt.subplot()
# 	sns.heatmap(results,annot=True,ax=ax,fmt='g')
# 	ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels')
# 	ax.set_title(f'Confusion Matrix: {argv[5]} {argv[6]} {argv[7]} model run') 
# 	ax.set_xticklabels(labels)
# 	ax.set_yticklabels(labels)
# 	print(results) 
# 	print ('Accuracy Score :',accuracy_score(actual_ls, predicted_ls))
# 	print ('Report : ')
# 	print (classification_report(actual_ls, predicted_ls))
# 	plt.show()
# 	plt.close('all')


# def create_zonal_stats_df(stem_raster,rgi_raster,shp,resolution,output_dir,boundary,zoom,pickle_dir,read_from_pickle,stat): 
# 	"""A helper function for calc_zonal_stats."""
# 	head,tail = os.path.split(stem_raster)

# 	if read_from_pickle.lower()=='true': 
# 		print('pickling...')
# 		#generate zonal stats and pickle
# 		output_file = pickle_dir+f'{tail[0:8]}_stem_rgi_zonal_stats_df'
# 		if os.path.exists(output_file): 
# 			pass
# 		else: 
# 			stem_df = calc_zonal_stats(stem_raster,shp,resolution,stat,'stem')
# 			rgi_df = calc_zonal_stats(rgi_raster,shp,resolution,stat,'rgi')
# 			df = pd.concat([stem_df,rgi_df],axis=1)
# 			pickle_data=pickle.dump(df, open(output_file, 'ab' ))
# 	elif os.path.exists(pickle_dir+f'{tail[0:8]}_stem_rgi_zonal_stats_df'): #read in the pickled df if its the same data
# 		print('reading from pickle...')
# 		df = pickle.load(open(pickle_dir+f'{tail[0:8]}_stem_rgi_zonal_stats_df','rb'))
# 	else: 
# 		print(pickle_dir+f'{tail[0:8]}_stem_rgi_zonal_stats_df'+'does not exist, please make sure the settings in the param file are correct')
# 	#calculate the percent error aggregated by cell (pixelwise doesn't make sense because its binary)
# 	df['pct_err'] = (((df['stem_count']-df['rgi_count'])/df['rgi_count'])*100)
# 	#rename a col to geometry because the plot function wants that
# 	df.rename(columns={'rgi_geometry':'geometry'},inplace=True)
# 	#get rid of garbage 
# 	df = df.drop(['stem_left','stem_top','stem_right','stem_bottom','stem_geometry','rgi_left','rgi_top','rgi_right','rgi_bottom'],axis=1)
# 	#select a subset by getting rid of infs
# 	df_slice = df.replace([np.inf, -np.inf],np.nan).dropna(axis=0)#df.query('stem_count!=0')#[df['stem_count']!=0 and df['rgi_count']!=0]
# 	#read in plotting shapefiles
# 	inset = gpd.read_file(boundary)
# 	background = gpd.read_file(zoom)
# 	#do the plotting 
# 	fig,ax = plt.subplots()
# 	#make the colorbar the same size as the plot
# 	divider = make_axes_locatable(ax)
# 	cax = divider.append_axes("right",size="5%",pad=0.1)
# 	left, bottom, width, height = [0.1, 0.525, 0.25, 0.25]
# 	ax1 = fig.add_axes([left, bottom, width, height])
# 	#specify discrete color ramp 
# 	cmap = mpl.colors.ListedColormap(['#005a32','#238443','#41ab5d','#78c679','#addd8e','#d9f0a3','#ffffcc',#'#ffffcc','#d9f0a3','#addd8e','#78c679','#41ab5d','#238443','#005a32',
# 	'#F2B701','#E73F74','#180600','#E68310','#912500','#CF1C90','#f23f01',
# 	'#f1eef6','#d0d1e6','#a6bddb','#74a9cf','#3690c0','#0570b0','#034e7b']) 
# 	#'#855C75','#D9AF6B','#AF6458','#736F4C','#526A83','#625377','#68855C','#9C9C5E','#A06177','#8C785D','#467378','#7C7C7C'])
# 	#'#5F4690','#1D6996','#38A6A5','#0F8554','#73AF48','#EDAD08','#E17C05','#CC503E','#94346E','#6F4070','#994E95','#666666'])#'#5D69B1','#52BCA3','#99C945','#CC61B0','#24796C','#DAA51B','#2F8AC4','#764E9F','#ED645A','#CC3A8E','#63665b'])#["#f5b460","#F1951C","#a86813", "#793200", "#004039","#006B5F", "#62BAAC","#ba6270"])#'#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a'])#
# 	norm = mpl.colors.BoundaryNorm([-100,-90,-80,-70,-60,-50,-40,-30,-20,-10,0,10,20,30,40,50,60,70,80,90,100],cmap.N) 
# 	background.plot(color='lightgray',ax=ax)
# 	df_slice.loc[df_slice['pct_err']<=100].plot(column='pct_err',ax=ax,legend=True,cax=cax,cmap=cmap,norm=norm)
# 	ax.set_title(f'{tail[0:8]} model run AK southern processing region percent error')

# 	inset.plot(color='lightgray',ax=ax1)
# 	df_slice.loc[df_slice['pct_err']<=100].plot(column='pct_err',ax=ax1,cmap=cmap,norm=norm)
# 	ax1.get_xaxis().set_visible(False)
# 	ax1.get_yaxis().set_visible(False)
# 	plt.tight_layout() 
# 	plt.show()
# 	plt.close('all')
# def select_random_pts_from_raster(self,target_value):
# 		"""From an input raster select a random subset of points (pixels)."""

# 		#read in the raster
# 		with rasterio.open(self.dif_raster) as src: 
# 			arr = xr.open_rasterio(src,chunks=10000)#1,(2917*6),(5761*6)))
# 			profile = src.profile

# 			print(arr.shape)
# 			if len(arr.shape) > 2: 
# 				arr = arr.squeeze(axis=0)
# 			else: 
# 				print('the shape of your array is: ', arr.shape)
# 			# print('exited if statement')
# 			print(arr.shape)
# 			print(type(arr))
# 			arr= arr.where(arr==1,drop=True)
			
# 			print(arr)
# 			new_arr = arr.stack(z=('x','y')).reset_index('z')
# 			new_arr = new_arr.dropna('z',how='any')
# 			#new_arr = new_arr.
# 			print('got below stack function')
# 			#new_arr = new_arr[new_arr.notnull()]
# 		# 	#print(arr.shape)
# 		# 	#arr = arr.where(arr==1,drop=True)
# 			print(new_arr)

# 			coords_list = []
# 			seed(1)
# 		# 	print(arr)
# 		# # 	print(arr.values.sum())
# 			#indices = [9*i + x for i, x in enumerate(sorted(np.random.choice(range(0,arr.shape[0]), 500,replace=False)))]
# 			indices = np.random.choice(range(0, new_arr.shape[0]), 500,replace=False)
# 			#x_vals = np.random.choice(x_arr,50)#range(0, arr.shape[0]), 500)
# 			#y_vals = np.random.choice(y_arr,50)
# 			#print(indices)
# 		# 	print(arr)
# 		# 	#rows = random.sample(range(0, arr.shape[0]), 100)
# 		# 	#for row,col in zip(rows,cols):
# 			for i in indices: 
# 				print(i,f'i is type {type(i)}')
# 				#print(arr[row,col].coords['band'].values)
# 				i = int(i)
# 				coords_list.append(tuple([float(new_arr[i].coords['y'].values),float(new_arr[i].coords['x'].values),float(new_arr[i].coords['band'].values)]))#append a tuple of coords in the form row,col
# 				#coords_list.append(tuple([float(arr[row,col].coords['y'].values),float(arr[row,col].coords['x'].values),float(arr[row,col].coords['band'].values)]))#append a tuple of coords in the form row,col
# 			df = pd.DataFrame(coords_list,columns=['lat','lon','value'])#{'lat':y_vals.tolist(),'lon':x_vals.tolist()},index=range(1,51))
# 			gdf = gpd.GeoDataFrame(df,geometry=gpd.points_from_xy(df.lon,df.lat))
# 			gdf.crs='EPSG:3338'
# 			gdf['id'] = range(1,gdf.shape[0]+1)
# 			#write out 
# 			out_filename = os.path.split(self.dif_raster)[1][:-4]
# 			#write a shapefile
# 			gdf.to_file(self.output_dir+out_filename+'.shp')#"/vol/v3/ben_ak/vector_files/glacier_outlines/revised_class_1_output_23.shp")#, driver='GeoJSON')
# 			gdf.to_csv(out_filename+'.csv')

# 			#print(gdf)
			# output_file = self.dif_raster[:-4]+'_subset_test.tif'
			# with rasterio.open(output_file, 'w', **profile) as dst: 
			# 	dst.write(arr)
		# 		# seed random number generator
		#print(coords_list[0])




# mask = None
		# with rasterio.Env():
		# 	with rasterio.open(self.input_raster) as src:
		# 		image = src.read(1).astype('int32') # first band
		# 		print(image.dtype)
		# 		results = (
		# 		{'properties': {'raster_val': v}, 'geometry': s}
		# 		for i, (s, v) 
		# 		in enumerate(
		# 			shapes(image, mask=mask, transform=src.transform)))

		# 		geoms = list(results)
		#  # first feature
		# print (geoms[0])
		# geoms = list(results)
		# gpd_polygonized_raster  = gpd.GeoDataFrame.from_features(geoms)
		# print(gpd_polygonized_raster)

# this allows GDAL to throw Python Exceptions
# 		gdal.UseExceptions()

# 		#
# 		#  get raster datasource
# 		#
# 		src_ds = gdal.Open(self.input_raster)
# 		print(src_ds)
# 		arr = np.array(src_ds.GetRasterBand(1).ReadAsArray()).astype('float32')
# 		arr[arr != 1] = np.nan
# 		print(arr)
# 		print(arr.shape)
# 		if src_ds is None:
# 		    print ('Unable to open %s' % src_filename)
# 		    sys.exit(1)

# 		try:
# 		    srcband = src_ds.GetRasterBand(1)

# 		except RuntimeError as e:
# 		    # for example, try GetRasterBand(10)
# 		   # print ('Band ( %i ) not found' % band_num)
# 		    print (e)
# 		    sys.exit(1)

# 		#
# 		#  create output datasource
# 		#layer = ds.CreateLayer(str(layer_name), spat_ref, ogr.wkbPolygon)


# #segimg=glob.glob('Poly.tif')[0]
# #src_ds = gdal.Open(segimg, GA_ReadOnly )
# #srcband=src_ds.GetRasterBand(1)
# #myarray=srcband.ReadAsArray() 
# #these lines use gdal to import an image. 'myarray' can be any numpy array

# 		mypoly=[]
# 		for vec in rasterio.features.shapes(arr):
# 		    mypoly.append(shape(vec[0])) #shape(vec[0])
# 		#from shapely.geometry import shape
# 		geom = []#[shape(i) for i in mypoly['geometry']]
# 		for i in mypoly: 
# 			i['geometry']=
# 		#print shape(geoms[0]['geometry'])
# 		print(mypoly[0])
# 		gpd_polygonized_raster = gpd.GeoDataFrame.from_features(mypoly)
# 		print(gpd_polygonized_raster.head())
# 		# tif_drv = gdal.GetDriverByName('GTiff') # create driver for writing geotiff file
# 		# outRaster = tif_drv.CreateCopy(str(self.input_raster[:-4]), self.input_raster , 0 )# create new copy of inut raster on disk
# 		# newBand = outRaster.GetRasterBand(1)  
# 		# poly_raster = newBand.WriteArray(arr) # write array data to this band 

# 		# dst_layername = self.output_dir+"polygonized_test_2"
# 		# drv = ogr.GetDriverByName("ESRI Shapefile")
# 		# dst_ds = drv.CreateDataSource( dst_layername + ".shp" )
# 		# dst_layer = dst_ds.CreateLayer(dst_layername, srs = None )

# 		# gdal.Polygonize(poly_raster, None, dst_layer, -1, [], callback=None )
# 		# print('finished

#print(arr)
			#arr_df = arr.to_dataframe('results').dropna('any')
			#rint(arr_df)
			#arr = arr.dropna(dim='x')
			#new_arr = arr_df.to_xarray()
			#print(new_arr)
			#print(arr_df.shape)
			#arr = arr.stack(z=('x', 'y'))
			#print(type(arr.coords))
			#print(arr.coords['x'][0])
			#band_arr = np.array(arr.coords['band'])
			#x_arr = np.array(arr.coords['x'])
			#y_arr = np.array(arr.coords['y'])
			#print(x_arr)
			#new_arr = np.stack(((arr.coords['y']),(arr.coords['x'])))
			#print(f'x arr shape is {x_arr.shape}')
			#print(f'y arr shape is {y_arr.shape}')
			#print(f'band shape is {band_arr.shape}')
			#print(x_arr)
			#new_arr = np.concatenate((y_arr,x_arr),axis=0)
			#print(new_arr.shape)
			#new_arr = arr.to_stacked_array('z')
			#print('made stacked array')
			#arr = arr.flatten()
			# print(f'here the arr shape is {arr.shape}')
			# arr = arr.reset_index('z')#.drop(['x', 'y'])
			#arr = arr.where(arr == 1, drop=True)

			#arr = arr[arr[:,:]==1]
			# print(arr.shape)
			#above here broken af
			#numpy test
			# arr = src.read()
			# profile = src.profile
			# arr = arr[arr == 1]
			# #arr = arr.where((arr==1,arr==1),drop=True)
			# #arr =arr[arr[:,0]==1, :]
			# #arr = arr[arr[:,:] == 1]

			# print(arr.shape)
			#arr = arr.squeeze(axis=0)
			#print(arr.shape)
			#arr = arr.flatten()
			#print(arr.shape)
			#print(arr.shape)
		# 	arr = xr.open_rasterio(src,(1,(2917*6),(5761*6)))
		# 	no_data=float((arr.attrs['nodatavals'])[0])
		# 	print(arr.shape)
		# 	#print(arr.chunks)

		# 	if len(arr.shape) > 2:
		# 	#remove the first axis 
		# 		arr = np.squeeze(arr,axis=0)
		# 	else: 
		# 		print('The shape of your array is: ', arr.shape)
		# 	#get some random points	to make coords indexers 
		# 	#arr = arr.where(arr == 1, drop=True)
		# 	#print(arr.shape)
		# 	#print(arr.chunks)
			#arr = arr.stack(dim_0=('x', 'y'))

			#arr = arr.reset_index('dim_0').drop(['x', 'y'])